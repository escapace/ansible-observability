#!/usr/bin/env bash

set -e
set -u
set -o pipefail

umask 027

SCRIPT_NAME=
SCRIPT_NAME="$(basename "$0")"

declare -A OBSERVABILITY_TAGS

TELEMETRY_ENVIRONMENT_FILE_PATH="/etc/sysconfig/telemetry-environment"

function wrap() {
  local strings="${1}"

  local length
  local cols
  local width

  length="$(echo -n "${strings}" | awk -F '|' '{print $1}' | awk '{ print length }' | sort -n | tail -1)"
  cols="$(tput cols)"

  if [[ $(((cols - 4) / length)) -ge 2 ]]; then
    if [[ $((cols - length - 4)) -ge 80 ]]; then
      width=80
    else
      width=$((cols - length - 4))
    fi

    echo -e "${strings}" |
      fold -s -w "${width}" |
      sed -e "/--/! s|^| \||g" |
      column -t -s '|' |
      sed 's/^/  /'
  else
    local lines
    local line
    readarray -t lines <<<"${strings}"
    local option
    local description

    if [[ "${cols}" -ge 80 ]]; then
      width="78"
    else
      width=$((cols - 2))
    fi

    for line in "${lines[@]}"; do
      option="${line%%|*}"
      description=${line#*|}

      echo "${option}"
      echo
      echo "${description}" | fold -s -w "${width}" | sed 's/^/  /'
      echo
    done
  fi
}

function print_usage() {
  echo
  echo "Usage: ${SCRIPT_NAME} [OPTIONS]"
  echo
  echo "This script is used to configure observability on a AWS EC2 server."
  echo
  echo "Options:"
  echo
  wrap "$(
    echo -e "--role|The instance role, one of \"server\", \"client\" or \"bastion\"."
    echo -e "--amp-workspace-id|Amazon Managed Service for Prometheus Workspace ID."
    echo -e "--cloudwatch-group-name|CloudWatch log group name."
    echo -e "--enable-cloudflared|Collect cloudflare tunnel client metrics."
    echo -e "--enable-consul|Collect consul metrics."
    echo -e "--enable-esm|Collect consul-esm metrics."
    echo -e "--enable-kresd|Collect knot resolver metrics."
    echo -e "--enable-newrelic|New Relic infrastructure agent and log collection."
    echo -e "--enable-nomad|Collect nomad metrics."
    echo -e "--enable-service-discovery|Discover scrape targets running on this node."
    echo -e "--enable-vault|Collect vault metrics."
    echo -e "--scrape-interval|The interval between scrapes, in seconds."
    echo -e "--secrets-bucket-name|AWS S3 secrets bucket name."
    echo -e "--tag|Tag vector logs and metrics. Repeat this option for additional tags."
  )"
  echo
}

function log() {
  local -r level="$1"
  local -r message="$2"
  local timestamp

  timestamp="$(date +"%Y-%m-%d %H:%M:%S")"

  echo >&2 -e "${timestamp} [${level}] [$SCRIPT_NAME] ${message}"
}

function log_info() {
  local -r message="$1"
  log "INFO" "$message"
}

function log_warn() {
  local -r message="$1"
  log "WARN" "$message"
}

function log_error() {
  local -r message="$1"
  log "ERROR" "$message"
}

function assert_not_empty() {
  local -r arg_name="$1"
  local -r arg_value="$2"

  if [[ -z "$arg_value" ]]; then
    log_error "The value for '$arg_name' cannot be empty"
    print_usage
    exit 1
  fi
}

function assert_is_installed() {
  local -r name="$1"

  if [[ ! $(command -v "${name}") ]]; then
    log_error "The binary '$name' is required by this script but is not installed or in the system's PATH."
    exit 1
  fi
}

function parse_tag() {
  local string="${1}"
  local -r regex="^([a-zA-Z_][a-zA-Z0-9_]*)=([a-zA-Z_:][a-zA-Z0-9_:]*)"
  local name
  local value

  if [[ "${string}" =~ ${regex} && "${#BASH_REMATCH[@]}" == "3" ]]; then
    name="${BASH_REMATCH[1]}"
    value="${BASH_REMATCH[2]}"
    OBSERVABILITY_TAGS["${name}"]="${value}"
  else
    log_error "Unable to parse tag: ${string}"
  fi
}

function join() {
  local d=${1-} f=${2-}

  if shift 2; then
    printf %s "$f" "${@/#/$d}"
  fi
}

function get_object_value() {
  local -r secrets_bucket_name="$1"
  local -r source="$2"

  local -r key="${EC2_INSTANCE_REGION}/${source}"

  log_info "Downloading ${key}"

  aws s3 cp --quiet \
    "s3://${secrets_bucket_name}/${key}" /dev/stdout || exit 1
}

function write_vector_configuration() {
  local -r enable_kresd="${1}"
  local -r enable_consul="${2}"
  local -r enable_nomad="${3}"
  local -r enable_cloudflared="${4}"
  local -r enable_newrelic="${5}"
  local -r enable_esm="${6}"
  local -r enable_vault="${7}"

  local -r vector_configuration_path="/etc/vector/vector.toml"
  local -r vector_telemetry_environment_path="/etc/systemd/system/vector.service.d/telemetry-environment.conf"

  local metrics_metadata_tags

  metrics_metadata_tags=$(
    for name in "${!OBSERVABILITY_TAGS[@]}"; do
      echo ".tags.${name}=\"${OBSERVABILITY_TAGS[$name]:-false}\""
    done
  )

  local logs_metadata_tags

  logs_metadata_tags=$(
    for name in "${!OBSERVABILITY_TAGS[@]}"; do
      echo ".${name}=\"${OBSERVABILITY_TAGS[$name]:-false}\""
    done
  )

  local vector_configuration_newrelic=""
  local vector_configuration_kresd=""
  local vector_configuration_consul=""
  local vector_configuration_nomad=""
  local vector_configuration_cloudflared=""
  local vector_configuration_esm=""
  local vector_configuration_vault=""
  local vector_sources_list_array=("\"metrics-amp-proxy\"" "\"metrics-node-exporter\"" "\"metrics-vector\"")

  if [[ "${enable_newrelic}" == "true" ]]; then
    vector_configuration_newrelic=$(
      cat <<EOF
[sinks.logs-newrelic-sink]
type = "new_relic"
inputs = [
  "intermediate-logs-enrich"
]

license_key = "\${NEWRELIC_LICENSE_KEY}"
account_id = "\${NEWRELIC_ACCOUNT_ID}"
region = "us"
compression = "gzip"
api = "logs"
batch.max_events = 50
healthcheck.enabled = false
request.concurrency = "adaptive"
EOF
    )

  fi

  if [[ "${enable_consul}" == "true" ]]; then
    vector_sources_list_array+=("\"metrics-consul*\"")

    vector_configuration_consul=$(
      cat <<EOF

## CONSUL

[sources.intermediate-metrics-consul]
type = "prometheus_scrape"
endpoints = ["http://[::1]:8500/v1/agent/metrics?format=prometheus"]
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[sources.intermediate-metrics-consul.auth]
token = "\${CONSUL_HTTP_TOKEN}"
strategy = "bearer"

[transforms.metrics-consul]
type = "remap"
inputs = ["intermediate-metrics-consul"]
source = '.tags.service = "consul"'

EOF
    )
  fi

  if [[ "${enable_esm}" == "true" ]]; then
    vector_sources_list_array+=("\"metrics-esm\"")

    vector_configuration_esm=$(
      cat <<EOF

## esm

[sources.intermediate-metrics-esm]
type = "prometheus_scrape"
endpoints = ["http://[::1]:8559/metrics"]
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[transforms.metrics-esm]
type = "remap"
inputs = ["intermediate-metrics-esm"]
source = '.tags.service = "esm"'

EOF
    )
  fi

  if [[ "${enable_kresd}" == "true" ]]; then
    vector_sources_list_array+=("\"metrics-kresd\"")

    vector_configuration_kresd=$(
      cat <<EOF

## KRESD

[sources.intermediate-metrics-kresd]
type = "prometheus_scrape"
endpoints = ["http://[::1]:8453/metrics"]
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[transforms.metrics-kresd]
type = "remap"
inputs = ["intermediate-metrics-kresd"]
source = '.tags.service = "kresd"'

EOF
    )
  fi

  if [[ "${enable_nomad}" == "true" ]]; then
    vector_sources_list_array+=("\"metrics-nomad\"")

    vector_configuration_nomad=$(
      cat <<EOF

## NOMAD

[sources.intermediate-metrics-nomad]
type = "prometheus_scrape"
# TODO: vector fails tls verification with [::1]
endpoints = ["https://localhost:4646/v1/metrics?format=prometheus"]
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[transforms.metrics-nomad]
type = "remap"
inputs = ["intermediate-metrics-nomad"]
source = '.tags.service = "nomad"'

EOF
    )
  fi

  if [[ "${enable_cloudflared}" == "true" ]]; then
    vector_sources_list_array+=("\"metrics-cloudflared\"")

    vector_configuration_cloudflared=$(
      cat <<EOF

## cloudflared

[sources.intermediate-metrics-cloudflared]
type = "prometheus_scrape"
endpoints = ["http://[::1]:9301/metrics"]
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[transforms.metrics-cloudflared]
type = "remap"
inputs = ["intermediate-metrics-cloudflared"]
source = '.tags.service = "cloudflared"'

EOF
    )
  fi

  if [[ "${enable_vault}" == "true" ]]; then
    vector_sources_list_array+=("\"metrics-vault\"")

    vector_configuration_vault=$(
      cat <<EOF

## vault

[sources.intermediate-metrics-vault]
type = "prometheus_scrape"
endpoints = ["https://localhost:8200/v1/sys/metrics?format=prometheus"]
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[transforms.metrics-vault]
type = "remap"
inputs = ["intermediate-metrics-vault"]
source = '.tags.service = "vault"'

EOF
    )
  fi

  local vector_sources_list
  vector_sources_list="$(join ", " "${vector_sources_list_array[@]}")"

  local -r vector_configuration=$(
    cat <<EOF
[healthchecks]
enabled = true
require_healthy = false

[api]
enabled = false
address = "127.0.0.1:8686"

# JOURNALD

[sources.logs-journald]
type = "journald"
current_boot_only = true

## NODE EXPORTER

[sources.intermediate-metrics-node-exporter]
type = "prometheus_scrape"
endpoints = ["http://[::1]:9100/metrics"]
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[transforms.metrics-node-exporter]
type = "remap"
inputs = ["intermediate-metrics-node-exporter"]
source = '.tags.service = "node-exporter"'

## VECTOR

[sources.intermediate-metrics-vector]
type = "internal_metrics"
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[transforms.metrics-vector]
type = "remap"
inputs = ["intermediate-metrics-vector"]
source = '.tags.service = "vector"'

## AMP-PROXY

[sources.intermediate-metrics-amp-proxy]
type = "prometheus_scrape"
endpoints = ["http://[::1]:9901/stats/prometheus"]
scrape_interval_secs = \${VECTOR_SCRAPE_INTERVAL:-60}

[transforms.metrics-amp-proxy]
type = "remap"
inputs = ["intermediate-metrics-amp-proxy"]
source = '.tags.service = "amp-proxy"'

${vector_configuration_kresd}
${vector_configuration_consul}
${vector_configuration_nomad}
${vector_configuration_cloudflared}
${vector_configuration_esm}
${vector_configuration_vault}

## ---

[transforms.intermediate-metrics-enrich]
type = "remap"
source = '''
.tags.region = get_env_var!("EC2_INSTANCE_REGION")
.tags.instance_id = get_env_var!("EC2_INSTANCE_ID")
.tags.instance_type = get_env_var!("EC2_INSTANCE_TYPE")
.tags.availability_zone = get_env_var!("EC2_INSTANCE_AVAILABILITY_ZONE")
del(.tags.host)
.tags.host = get_hostname!()
${metrics_metadata_tags}
'''
inputs = [${vector_sources_list}]

[transforms.intermediate-logs-enrich]
type = "remap"
source = '''
.region = get_env_var!("EC2_INSTANCE_REGION")
.instance_id = get_env_var!("EC2_INSTANCE_ID")
.instance_type = get_env_var!("EC2_INSTANCE_TYPE")
.availability_zone = get_env_var!("EC2_INSTANCE_AVAILABILITY_ZONE")
del(.host)
.host = get_hostname!()
${logs_metadata_tags}
'''
inputs = [
  "logs-journald"
]

## ---

[transforms.intermediate-metrics-filter]
type = "filter"
inputs = [
  "intermediate-metrics-enrich"
]
condition.type = "vrl"
condition.source = '''
  if exists(.name) {
    if (starts_with(.name, "envoy_") ?? false) {
      match_any(.name, [
        r'^envoy_cluster_upstream_cx_active',
        r'^envoy_cluster_upstream_cx_connect_fail',
        r'^envoy_cluster_upstream_cx_no_route',
        r'^envoy_cluster_upstream_cx_rx_bytes_total',
        r'^envoy_cluster_upstream_cx_total',
        r'^envoy_cluster_upstream_cx_tx_bytes_total',
        r'^envoy_cluster_upstream_rq_completed',
        r'^envoy_cluster_upstream_rq_time',
        r'^envoy_cluster_upstream_rq_total',
        r'^envoy_cluster_upstream_rq_xx',
        r'^envoy_http_downstream_cx_active',
        r'^envoy_http_downstream_cx_rx_bytes_total',
        r'^envoy_http_downstream_cx_total',
        r'^envoy_http_downstream_cx_tx_bytes_total',
        r'^envoy_http_downstream_rq_completed',
        r'^envoy_http_downstream_rq_time',
        r'^envoy_http_downstream_rq_total',
        r'^envoy_http_downstream_rq_xx',
        r'^envoy_listener_http_downstream_rq_xx',
        r'^envoy_tcp_downstream_cx_connect_fail',
        r'^envoy_tcp_downstream_cx_no_route',
        r'^envoy_tcp_downstream_cx_rx_bytes_total',
        r'^envoy_tcp_downstream_cx_total',
        r'^envoy_tcp_downstream_cx_tx_bytes_total'
      ]) ?? false
    } else if (starts_with(.name, "process_") ?? false) {
      match_any(.name, [
        r'^process_cpu_seconds_total',
        r'^process_max_fds',
        r'^process_open_fds',
        r'^process_resident_memory_bytes',
        r'^process_start_time_seconds',
        r'^process_virtual_memory_bytes',
        r'^process_virtual_memory_max_bytes'
      ]) ?? false
    } else if (starts_with(.name, "go_") ?? false) {
      match_any(.name, [
        r'^go_gc_duration_seconds\$',
        r'^go_gc_duration_seconds_sum',
        r'^go_goroutines',
        r'^go_memstats_alloc_bytes',
        r'^go_memstats_alloc_bytes_total',
        r'^go_memstats_heap_inuse_bytes',
        r'^go_memstats_stack_inuse_bytes'
      ]) ?? false
    } else if (.tags.service == "node-exporter") {
      match_any(.name, [
        r'^node_context_switches_total',
        r'^node_cpu_seconds_total',
        r'^node_disk_discard_time_seconds_total',
        r'^node_disk_discards_completed_total',
        r'^node_disk_discards_merged_total',
        r'^node_disk_io_now',
        r'^node_disk_io_time_seconds_total',
        r'^node_disk_io_time_weighted_seconds_total',
        r'^node_disk_read_bytes_total',
        r'^node_disk_read_time_seconds_total',
        r'^node_disk_reads_completed_total',
        r'^node_disk_reads_merged_total',
        r'^node_disk_write_time_seconds_total',
        r'^node_disk_writes_completed_total',
        r'^node_disk_writes_merged_total',
        r'^node_disk_written_bytes_total',
        r'^node_entropy_available_bits',
        r'^node_filesystem_avail_bytes',
        r'^node_filesystem_device_error',
        r'^node_filesystem_free_bytes',
        r'^node_filesystem_readonly',
        r'^node_filesystem_size_bytes',
        r'^node_forks_total',
        r'^node_intr_total',
        r'^node_load1',
        r'^node_load15',
        r'^node_load5',
        r'^node_memory_Buffers_bytes',
        r'^node_memory_Cached_bytes',
        r'^node_memory_HardwareCorrupted_bytes',
        r'^node_memory_MemFree_bytes',
        r'^node_memory_MemTotal_bytes',
        r'^node_memory_PageTables_bytes',
        r'^node_memory_SReclaimable_bytes',
        r'^node_memory_Slab_bytes',
        r'^node_memory_SwapCached_bytes',
        r'^node_memory_SwapFree_bytes',
        r'^node_memory_SwapTotal_bytes',
        r'^node_network_receive_bytes_total',
        r'^node_network_receive_drop_total',
        r'^node_network_receive_errs_total',
        r'^node_network_receive_packets_total',
        r'^node_network_transmit_bytes_total',
        r'^node_network_transmit_drop_total',
        r'^node_network_transmit_errs_total',
        r'^node_network_transmit_packets_total',
        r'^node_processes_max_threads',
        r'^node_processes_state',
        r'^node_processes_threads',
        r'^node_procs_blocked',
        r'^node_procs_running',
        r'^node_schedstat_running_seconds_total',
        r'^node_schedstat_timeslices_total',
        r'^node_schedstat_waiting_seconds_total',
        r'^node_scrape_collector_duration_seconds',
        r'^node_systemd_units',
        r'^node_timex_estimated_error_seconds',
        r'^node_timex_maxerror_seconds',
        r'^node_timex_offset_seconds'
      ]) ?? false
    } else if (.tags.service == "kresd") {
      match_any(.name, [
        r'^kresd_answer_cached',
        r'^kresd_answer_nodata',
        r'^kresd_answer_noerror',
        r'^kresd_answer_nxdomain',
        r'^kresd_answer_servfail',
        r'^kresd_answer_total',
        r'^kresd_cache_usage_percent',
        r'^kresd_latency',
        r'^kresd_request_total'
      ]) ?? false
    } else if (.tags.service == "vector") {
      match_any(.name, [
        r'^component_received_events_total',
        r'^component_sent_events_total',
        r'^http_client_errors_total',
        r'^http_client_rtt_seconds',
        r'^utilization'
      ]) ?? false
    } else if (.tags.service == "cloudflared") {
      match_any(.name, [
        r'^cloudflared_tunnel_request_errors',
        r'^cloudflared_tunnel_response_by_code',
        r'^cloudflared_tunnel_server_locations',
        r'^cloudflared_tunnel_total_requests',
        r'^cloudflared_tunnel_tunnel_authenticate_success',
        r'^cloudflared_tunnel_tunnel_register_fail',
        r'^cloudflared_tunnel_tunnel_register_success',
        r'^quic_client_latest_rtt',
        r'^quic_client_min_rtt',
        r'^quic_client_receive_packets',
        r'^quic_client_sent_packets',
        r'^quic_client_smoothed_rtt'
      ]) ?? false
    } else {
      true
    }
  } else {
    false
  }
'''

[sinks.metrics-amp-sink]
type = "prometheus_remote_write"
endpoint = "http://[::1]:9201/api/v1/remote_write"
healthcheck = false
request.concurrency = "adaptive"
inputs = [
  "intermediate-metrics-filter"
]

[sinks.logs-cloudwatch-sink]
type = "aws_cloudwatch_logs"
create_missing_group = true
create_missing_stream = true
group_name = "\${AWS_CLOUDWATCH_GROUP_NAME}"
compression = "gzip"
region = "${EC2_INSTANCE_REGION}"
encoding.codec = "json"
stream_name = "{{ instance_id }}"
request.concurrency = "adaptive"
inputs = [
  "intermediate-logs-enrich"
]

$vector_configuration_newrelic

EOF
  )

  log_info "Writing ${vector_configuration_path}"

  echo -e "${vector_configuration}" >"${vector_configuration_path}"

  chown vector:vector "${vector_configuration_path}"
  chmod 640 "${vector_configuration_path}"

  mkdir -p "$(dirname "${vector_telemetry_environment_path}")"
  chown root:root "$(dirname "${vector_telemetry_environment_path}")"
  chmod 755 "$(dirname "${vector_telemetry_environment_path}")"

  log_info "Writing ${vector_telemetry_environment_path}"

  local -r vector_telemetry_environment=$(
    cat <<EOF
[Unit]
After=consul-online.target
Wants=consul-online.target
After=vault-online.target
Wants=vault-online.target
After=nomad-online.target
Wants=nomad-online.target
After=kresd.target
Wants=kresd.target
After=node-exporter.service
Wants=node-exporter.service

[Service]
EnvironmentFile=${TELEMETRY_ENVIRONMENT_FILE_PATH}
EOF
  )

  echo -e "${vector_telemetry_environment}" >"${vector_telemetry_environment_path}"
  chown root:root "${vector_telemetry_environment_path}"
  chmod 644 "${vector_telemetry_environment_path}"
}

function write_telemetry_environment() {
  local -r role="${1}"
  local -r secrets_bucket_name="${2}"
  local -r scrape_interval="${3}"
  local -r cloudwatch_group_name="${4}"
  local -r enable_newrelic="${5}"

  local -r telemetry_environment_file_path="${TELEMETRY_ENVIRONMENT_FILE_PATH}"

  local aws_access_key_id
  local aws_secret_access_key

  aws_access_key_id="$(get_object_value "${secrets_bucket_name}" "${role}/telemetry/aws-access-key-id-telemetry")"
  aws_secret_access_key="$(get_object_value "${secrets_bucket_name}" "${role}/telemetry/aws-secret-access-key-telemetry")"

  local consul_http_token

  consul_http_token="$(get_object_value "${secrets_bucket_name}" "${role}/telemetry/consul-acl-token-consul-telemetry")"

  local telemetry_newrelic_environment=""

  if [[ "${enable_newrelic}" == "true" ]]; then
    local newrelic_account_id=""
    local newrelic_license_key=""

    newrelic_account_id="$(get_object_value "${secrets_bucket_name}" "${role}/telemetry/newrelic-account-id")"
    newrelic_license_key="$(get_object_value "${secrets_bucket_name}" "${role}/telemetry/newrelic-license-key")"

    telemetry_newrelic_environment=$(
      cat <<EOF
NEWRELIC_ACCOUNT_ID=${newrelic_account_id}
NEWRELIC_LICENSE_KEY=${newrelic_license_key}
EOF
    )

    local -r newrelic_infra_file_path="/etc/newrelic-infra.yml"

    local -r newrelic_infra=$(
      cat <<EOF
license_key: "${newrelic_license_key}"
enable_process_metrics: false
selinux_enable_semodule: false
dns_hostname_resolution: false
metrics_nfs_sample_rate: -1
network_interface_filters:
  prefix:
    - dummy
    - lo
    - vmnet
    - sit
    - tun
    - tap
    - vmnet
    - veth
    - docker
    - nomad
  index-1:
    - tun
    - tap
EOF
    )

    log_info "Writing ${newrelic_infra_file_path}"

    echo -e "${newrelic_infra}" >"${newrelic_infra_file_path}"

    chown root:root "${newrelic_infra_file_path}"
    chmod 640 "${newrelic_infra_file_path}"
  fi

  local -r telemetry_environment=$(
    cat <<EOF
AWS_ACCESS_KEY_ID=${aws_access_key_id}
AWS_SECRET_ACCESS_KEY=${aws_secret_access_key}
AWS_CLOUDWATCH_GROUP_NAME=${cloudwatch_group_name}
VECTOR_SCRAPE_INTERVAL=${scrape_interval}
CONSUL_HTTP_TOKEN=${consul_http_token}
${telemetry_newrelic_environment}
EOF
  )

  log_info "Writing ${telemetry_environment_file_path}"

  echo -e "${telemetry_environment}" >"${telemetry_environment_file_path}"

  chown root:root "${telemetry_environment_file_path}"
  chmod 640 "${telemetry_environment_file_path}"

  setfacl -m u:vector:r "${telemetry_environment_file_path}"
  setfacl -m u:amp-proxy:r "${telemetry_environment_file_path}"
}

function write_amp_proxy_configuration() {
  local -r workspace_id="${1}"
  local -r region="${EC2_INSTANCE_REGION}"

  local -r amp_proxy_configuration_path="/opt/amp-proxy/config/default.yml"
  local -r amp_proxy_service_configuration_path="/etc/systemd/system/amp-proxy.service"

  local -r amp_proxy_configuration=$(
    cat <<EOF
admin:
  address:
    socket_address: { address: "::1", port_value: 9901 }

static_resources:
  listeners:
    - address:
        socket_address:
          protocol: "TCP"
          address: "::1"
          port_value: 9201
      filter_chains:
        - filters:
            - name: envoy.filters.network.http_connection_manager
              typed_config:
                "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                codec_type: AUTO
                stat_prefix: amp-proxy
                strip_any_host_port: true
                route_config:
                  name: local_route
                  virtual_hosts:
                    - name: amp-proxy
                      domains: ['*']
                      routes:
                        - name: amp-proxy
                          match:
                            prefix: /
                          route:
                            cluster: amp-proxy
                http_filters:
                  - name: envoy.filters.http.lua
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua
                      inline_code: |
                        -- Called on the request path.
                        function envoy_on_request(request_handle)
                          local path = request_handle:headers():get(":path")
                          request_handle:headers():replace(":path", "/workspaces/${workspace_id}" .. path)
                        end
                        -- Called on the response path.
                        function envoy_on_response(response_handle)
                        end
                  - name: envoy.filters.http.aws_request_signing
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.http.aws_request_signing.v3.AwsRequestSigning
                      service_name: aps
                      region: ${region}
                      host_rewrite: aps-workspaces.${region}.amazonaws.com
                      match_excluded_headers:
                        - prefix: x-envoy
                        - prefix: x-forwarded
                        - prefix: cf-
                        - prefix: sec-
                        - exact: upgrade-insecure-requests
                        - exact: user-agent
                        - exact: cookie
                        - exact: priority
                        - exact: cdn-loop
                        - exact: x-amzn-trace-id

                  - name: envoy.filters.http.router
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
  clusters:
    - name: amp-proxy
      connect_timeout: 0.25s
      type: LOGICAL_DNS
      lb_policy: ROUND_ROBIN
      transport_socket:
        name: envoy.transport_sockets.tls
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
      load_assignment:
        cluster_name: amp-proxy
        endpoints:
          - lb_endpoints:
              - endpoint:
                  address:
                    socket_address:
                      address: aps-workspaces.${region}.amazonaws.com
                      port_value: 443
EOF
  )

  local -r amp_proxy_service_configuration=$(
    cat <<EOF
[Unit]
Description=Amazon Managed Service for Prometheus Signing Proxy
After=network-online.target
Wants=network-online.target

[Service]
User=amp-proxy
Group=amp-proxy
EnvironmentFile=${TELEMETRY_ENVIRONMENT_FILE_PATH}
ExecStart=/usr/bin/envoy --use-dynamic-base-id --disable-hot-restart --log-level error --config-path "${amp_proxy_configuration_path}"
KillMode=mixed
LimitNOFILE=640000
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
WantedBy=vector.service

EOF
  )

  log_info "Writing ${amp_proxy_configuration_path}"
  log_info "Writing ${amp_proxy_service_configuration_path}"

  echo -e "$amp_proxy_configuration" >"$amp_proxy_configuration_path"
  chown amp-proxy:amp-proxy "${amp_proxy_configuration_path}"
  chmod 644 "${amp_proxy_configuration_path}"

  echo -e "$amp_proxy_service_configuration" >"$amp_proxy_service_configuration_path"
  chown root:root "${amp_proxy_service_configuration_path}"
  chmod 644 "${amp_proxy_service_configuration_path}"
}

function write_service_discovery_configuration() {
  local -r vector_service_discovery_service_path="/etc/systemd/system/vector-service-discovery.service"
  local -r vector_service_discovery_template_path="/etc/vector/service-disccovery.ctmpl"
  local -r vector_service_discovery_configuration_path="/etc/vector/service-disccovery.toml"

  local -r vector_service_discovery_service=$(
    cat <<EOF
[Unit]
Description=Vector Service Discovery
After=network-online.target
Wants=network-online.target
After=vector.service
Wants=vector.service
After=consul-online.target
Wants=consul-online.target

[Service]
User=vector
Group=vector
EnvironmentFile=${TELEMETRY_ENVIRONMENT_FILE_PATH}
ExecStart=consul-template -log-level=err -reload-signal=SIGHUP -kill-signal=SIGINT -consul-retry -consul-retry-attempts=0 -consul-retry-backoff=250ms -consul-retry-max-backoff=0 -template="${vector_service_discovery_template_path}:${vector_service_discovery_configuration_path}"
ExecReload=/bin/kill -HUP \$MAINPID
KillSignal=SIGINT
LimitNOFILE=640000
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target

EOF
  )

  local -r vector_service_discovery_template=$(
    cat <<'EOF'
{{- with node -}}
{{- range .Services -}}
{{- if and (index .Meta "prometheus_endpoints") (.Service | regexMatch "-sidecar-proxy" | not) -}}
{{- $service := . -}}
{{- $value :=  sprig_compact (sprig_uniq (.Meta.prometheus_endpoints | parseJSON)) -}}
{{- range $value -}}
{{- scratch.MapSet "endpoints" . $service -}}
{{- end -}}
{{- end -}}
{{- end -}}
{{- end -}}

{{ range sprig_keys (scratch.Get "endpoints") }}
{{- $service := index (scratch.Get "endpoints") . -}}
{{- $name := sprig_list (sprig_kebabcase $service.Service) (sprig_sha256sum .) | sprig_join "-" -}}
{{- $scrape_interval := sprig_default "${VECTOR_SCRAPE_INTERVAL:-60}" $service.Meta.prometheus_scrape_interval -}}
{{- $tags := sprig_default "{}" $service.Meta.prometheus_labels | parseJSON -}}
# {{ $name }}

[sources.intermediate-metrics-consul-{{ $name }}]
type = "prometheus_scrape"
endpoints = [{{ . | toJSON }}]
scrape_interval_secs = {{ $scrape_interval }}

[transforms.metrics-consul-{{ $name }}]
type = "remap"
inputs = ["intermediate-metrics-consul-{{ $name }}"]
source = '''
.tags.service = "{{ $service.Service }}"
{{ range $key, $value := $tags -}}{{- if not (sprig_empty $value) -}}
.tags.{{ sprig_snakecase $key }} = "{{ $value | sprig_toString }}"
{{ end -}}{{- end -}}
'''

{{ end }}

EOF
  )

  echo -e "$vector_service_discovery_service" >"$vector_service_discovery_service_path"
  chown root:root "${vector_service_discovery_service_path}"
  chmod 644 "${vector_service_discovery_service_path}"

  echo -e "$vector_service_discovery_template" >"$vector_service_discovery_template_path"
  chown vector:vector "${vector_service_discovery_template_path}"
  chmod 640 "${vector_service_discovery_template_path}"
}

function configure_node_exporter() {
  local node_exporter_options_path="/opt/node-exporter/config/options"

  # '--collector.conntrack'

  local node_exporter_options_array=(
    '--collector.cpu'
    '--collector.disable-defaults'
    '--collector.diskstats'
    '--collector.diskstats.device-exclude="^(dm-|ram|loop|fd|(h|s|v|xv)d[a-z]|nvme[0-9]+n[0-9]+p)[0-9]+\$"'
    '--collector.entropy'
    '--collector.filesystem'
    '--collector.filesystem.fs-types-exclude="^(tmpfs|systemd-1|efivarfs|autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)\$"'
    '--collector.filesystem.mount-points-exclude="^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+)(\$|/)"'
    '--collector.loadavg'
    '--collector.meminfo'
    '--collector.netclass'
    '--collector.netclass.ignored-devices="^(nomad|docker.+|lo|dummy.+|veth.+)\$"'
    '--collector.netdev'
    '--collector.netdev.device-exclude="^(nomad|docker.+|lo|dummy.+|veth.+)\$"'
    '--collector.pressure'
    '--collector.processes'
    '--collector.schedstat'
    '--collector.stat'
    '--collector.systemd'
    '--collector.systemd.unit-include="(nomad.service|consul.service|amp-proxy.service|vector.service|kresd@.+|consul-terraform-sync.service|sshd.service|vector-service-discovery.service|ferm.service|ec2-environment.service|storage-setup.service|docker.service|consul-online.service|vault.service|vault-online.service|node-exporter.service)"'
    '--collector.timex'
    '--collector.vmstat'
    '--web.listen-address="[::1]:9100"'
  )

  local node_exporter_options
  node_exporter_options="OPTIONS=$(join " " "${node_exporter_options_array[@]}")"

  echo -e "$node_exporter_options" >"${node_exporter_options_path}"
  chown node-exporter:node-exporter "${node_exporter_options_path}"
  chmod 640 "${node_exporter_options_path}"
}

function run() {
  if [[ ! -f "/etc/sysconfig/ec2-environment" ]]; then
    print_usage

    log_error "/etc/sysconfig/ec2-environment: No such file"

    exit 1
  fi

  set -o allexport
  source "/etc/sysconfig/ec2-environment"
  set +o allexport

  local role=""
  local secrets_bucket_name=""
  local cloudwatch_group_name=""
  local amp_workspace_id=""
  local scrape_interval="60"
  local enable_kresd="false"
  local enable_consul="false"
  local enable_esm="false"
  local enable_nomad="false"
  local enable_vault="false"
  local enable_cloudflared="false"
  local enable_service_discovery="false"
  local enable_newrelic="false"

  while [[ $# -gt 0 ]]; do
    local key="$1"

    case "$key" in
    --role)
      assert_not_empty "$key" "$2"
      role="$2"
      shift
      ;;
    --scrape-interval)
      assert_not_empty "$key" "$2"
      scrape_interval="$2"
      shift
      ;;
    --secrets-bucket-name)
      assert_not_empty "$key" "$2"
      secrets_bucket_name="$2"
      shift
      ;;
    --amp-workspace-id)
      assert_not_empty "$key" "$2"
      amp_workspace_id="$2"
      shift
      ;;
    --cloudwatch-group-name)
      assert_not_empty "$key" "$2"
      cloudwatch_group_name="$2"
      shift
      ;;
    --tag)
      assert_not_empty "$key" "$2"
      parse_tag "${2}"
      shift
      ;;
    --enable-kresd)
      enable_kresd=true
      ;;
    --enable-consul)
      enable_consul=true
      ;;
    --enable-esm)
      enable_esm=true
      ;;
    --enable-nomad)
      enable_nomad=true
      ;;
    --enable-vault)
      enable_vault=true
      ;;
    --enable-cloudflared)
      enable_cloudflared=true
      ;;
    --enable-service-discovery)
      enable_service_discovery=true
      ;;
    --enable-newrelic)
      enable_newrelic=true
      ;;
    --help)
      print_usage
      exit
      ;;
    *)
      log_error "Unrecognized argument: $key"
      print_usage
      exit 1
      ;;
    esac

    shift
  done

  assert_not_empty "--secrets-bucket-name" "${secrets_bucket_name}"
  assert_not_empty "--cloudwatch-group-name" "${cloudwatch_group_name}"
  assert_not_empty "--amp-workspace-id" "${amp_workspace_id}"
  assert_not_empty "--role" "$role"

  if ! [[ "$role" == "server" || "$role" == "client" || "$role" == "bastion" ]]; then
    log_error "Unrecognized value for the --role flag."
    exit 1
  fi

  if ! [[ "${scrape_interval}" =~ ^[0-9]+$ ]]; then
    log_error "--scrape-interval must be a number."
    print_usage
    exit 1
  fi

  assert_is_installed "systemctl"
  assert_is_installed "jq"
  assert_is_installed "awk"

  if [[ "${enable_service_discovery}" == "true" ]]; then
    assert_is_installed "consul-template"
  fi

  configure_node_exporter

  write_telemetry_environment \
    "${role}" \
    "${secrets_bucket_name}" \
    "${scrape_interval}" \
    "${cloudwatch_group_name}" \
    "${enable_newrelic}"

  write_amp_proxy_configuration \
    "${amp_workspace_id}"

  write_vector_configuration \
    "${enable_kresd}" \
    "${enable_consul}" \
    "${enable_nomad}" \
    "${enable_cloudflared}" \
    "${enable_newrelic}" \
    "${enable_esm}" \
    "${enable_vault}"

  if [[ "${enable_service_discovery}" == "true" ]]; then
    write_service_discovery_configuration
  fi

  systemctl daemon-reload

  local services=()

  services+=("node-exporter.service")
  services+=("vector.service")
  services+=("amp-proxy.service")

  if [[ "${enable_service_discovery}" == "true" ]]; then
    services+=("vector-service-discovery.service")
  fi

  if [[ "${enable_newrelic}" == "true" ]]; then
    services+=("newrelic-infra.service")
  fi

  systemctl --no-block enable "${services[@]}"
  systemctl --no-block restart "${services[@]}"
}

run "$@"
